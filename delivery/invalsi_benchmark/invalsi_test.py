"""
    This script defines a benchmark for evaluating the performance of a Large Language Model (LLM)
    on a dataset of INVALSI tests, an italian school-grade educational assessment tests. 
    
    We chose to use Italian language and comprehension INVALSI test to measure how well an LLM 
    perform on the Italian language.
"""

import datetime
import gc
import torch
import transformers
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
import os
import json
import shutil
from rouge_score import rouge_scorer

"""
    This function calculates the ROUGE-1 precision score between a reference answer (ref_answer) 
    and the answer generated by the LLM (llm_answer).
    
    ROUGE-1 focuses on the overlap of unigrams (single words) between the two texts.
    
    use_stemmer=True indicates that words are stemmed before comparison (e.g., "running" and 
    "ran" are considered the same).
"""
def calculate_rouge_1(ref_answer, llm_answer):
    scorer = rouge_scorer.RougeScorer(["rouge1"], use_stemmer=True)
    scores = scorer.score(ref_answer, llm_answer)
    return scores["rouge1"].precision

# Parse and load the JSON files content 
def parse_json_files(folder_path):
    data = {}
    for filename in os.listdir(folder_path):
        if filename.endswith(".json"):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, "r") as f:
                try:
                    data[filename] = json.load(f)
                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON in file {filename}: {e}")
    return data


print("=" * shutil.get_terminal_size().columns)
print("Start parsing INVALSI json file...\n")

"""
    Path to a folder containing the json files.
    The json files contains only multiple-choice answers
"""
folder_path = "/home/bruno/Documents/GitHub/ir-nlp-llama/delivery/invalsi_benchmark/json_tests"
parsed_data = parse_json_files(folder_path)

print(f"Loaded {len(parsed_data)} INVALSI test\n")
print("=" * shutil.get_terminal_size().columns)

# Choose the model to evaluate

# base_model = "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA"
# model_name = "LLaMAntino-3"

# base_model = "meta-llama/Meta-Llama-3-8B-Instruct"
# model_name = "Llama-3"

# Our model
base_model = "/home/bruno/Documents/GitHub/ir-nlp-llama/delivery/model_dump/Formal_LLaMAntino_3" # Change this path to the model dump
model_name = "Wiki-LLaMAntino"

# Configuration parmas
top_p = 0.95
temp = 0.6
n_token_out = 4 # Reduced output tokens, generate just the answer id 
num_test_epoch = 2 # Number of test runs, the final metrci is the overall mean
num_few_shots = 2 # Number of few shots to use in the prompt

print(f"Initialize on model: f{model_name}")
print(f"Model HF path: f{base_model}")
print("=" * shutil.get_terminal_size().columns)
print("Start loading model...")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, # Use 4-bit quantization for efficency
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=False,
)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(base_model)

print("Model loaded successfully")
print("=" * shutil.get_terminal_size().columns)
print("\n")

print("=" * shutil.get_terminal_size().columns)
print("Start benchmark")

"""
    Maximum two shots.
    The shots does not use full context to avoid OOM errors, though they provide good performance.
    
    We found that LLaMA-derived models thend to respond with the letter linked to the response with a dot.
    We chose to accommodate this behavior.
"""
few_shots = [
    # Few Shot 1
    """
    Contesto: 
        Il racconto narra la storia di un'amicizia estiva tra due ragazzi che si vedono solo durante i due mesi di vacanza in una città di mare. Il narratore si interroga sulla natura del loro rapporto, definito amicizia per convenienza sociale, ma in realtà caratterizzato da distacco durante il resto dell'anno. Si evidenzia la differenza tra la percezione dell'amicizia quando sono insieme agli altri e quando sono soli. L'arrivo annuale dell'amico, sempre il primo luglio, viene descritto come un momento cruciale, fino a quando un anno l'amico non arriva, generando ansia e preoccupazione nel narratore. L'incontro avviene poi in modo diverso dal solito, con l'amico impegnato ad aiutare il padre a scaricare i bagagli, mostrando stanchezza e dispiacere per il tempo sottratto alla vacanza. Il racconto si conclude con una riflessione sulla natura del loro rapporto, ormai concluso, e sul ricordo della fatica di mantenere un'amicizia così impegnativa.

    Domanda:
        Secondo il narratore, perché l'amico, scaricando i bagagli, ha pianto?
        A. Ha litigato a lungo con il padre
        B. È stato costretto a fare un lavoro che non gli piace
        C. Il padre è molto severo con lui
        D. Gli è stato sottratto del tempo riservato alla vacanza
    
    D.
    """,
    # Few Shot 2
    """
    Contesto:
        Il testo descrive la fragilità degli ecosistemi forestali tropicali, in particolare il suolo povero di nutrienti e la sua vulnerabilità alla deforestazione. La vegetazione rigogliosa è possibile grazie a una fitta rete di radici superficiali che assorbono rapidamente i materiali organici in decomposizione. La deforestazione espone il suolo all'azione erosiva della pioggia e del sole, portando alla perdita di humus, alla diminuzione della capacità di trattenere acqua e alla desertificazione. Il testo evidenzia anche l'effetto sulla gestione delle acque, con alternanza di siccità e inondazioni a causa della mancanza della "spugna" radicale che assorbe l'acqua piovana.

    Domanda:
        Nella frase "adattatisi a ruoli molto particolari" (riga 5), puoi sostituire "adattatisi" con:
        A. poiché si sono adattati
        B. prima che si siano adattati
        C. che si sono adattati
        D. nello stesso tempo si sono adattati
    
    C.
    """
]

def run_invalsi_test():
    total_rouge: float = 0.0
    question_count: int = 0
    for file_name in parsed_data:
        print(f" Working file {file_name}")
        for invalsi_test in parsed_data[file_name]:
            test_context = invalsi_test["contesto"]
            for questions in invalsi_test["domande"]:
                test_question = questions["prompt_domanda"]
                right_answer = questions["risposta_corretta"]

                # Build the example string to embed it in the model prompt 
                prompt_shots = ""
                if num_few_shots > 0:
                    shots = "\n\n".join(few_shots[:num_few_shots])
                    prompt_shots = f"""
                    Inizio Esempi
                        {shots}
                    Fine Esempi
                    """

                # Complete model prompt
                model_prompt = f"""
                    Informazioni: 
                        Rispondi alla seguente domanda.
                        Non indicare voci correlate e altre informazioni.
                        Fornisci in output solo la lettera associata alla risposta corretta.
                        
                    Esempio di output generato: "A.", "B.", "C." o "D."
                    
                    {prompt_shots}
                    
                    Rispondi alla seguente domanda.
                    
                    Contesto: {test_context}
                    
                    Domanda: {test_question}
                """
                
                # print(f"Generated prompt:\t{model_prompt}")

                pipe = transformers.pipeline(
                    model=model,
                    tokenizer=tokenizer,
                    return_full_text=False,  # langchain expects the full text
                    task="text-generation",
                    max_new_tokens=n_token_out,  # max number of tokens to generate in the output
                    temperature=temp,  # temperature for more or less creative answers
                    do_sample=True,
                    top_p=top_p,
                )
                llm_reponse = pipe(model_prompt)
                full_reponse = ""
                for seq in llm_reponse:
                    full_reponse = full_reponse + seq["generated_text"]
                
                # Clean model response    
                full_reponse = full_reponse.strip()
                full_reponse = full_reponse.replace(".", "")
                
                question_count += 1
                question_rouge = calculate_rouge_1(
                    ref_answer=right_answer, llm_answer=full_reponse
                )
                total_rouge += question_rouge
                print(f"""
                    LLM Response:\t{full_reponse}
                    REAL Reponse:\t{right_answer}
                    ROUGE-1 Score: {question_rouge}
                    AVERAGE ROUGE-1 Score: {total_rouge/question_count}
                """)
                
                del pipe
                del full_reponse
                del llm_reponse
                gc.collect()
                torch.cuda.empty_cache() 

    return {"rouge": total_rouge / question_count, "count": question_count}


result = {}
total_rouge: float = 0.0
question_count: int = 0

# Run the test num_test_epoch times
for i in range(0, num_test_epoch):
    print(f"Test epoch {i+1}")
    run_result = run_invalsi_test()
    total_rouge += run_result["rouge"]
    question_count += run_result["count"]
    print("=" * shutil.get_terminal_size().columns)
    print(f"""
            Epoch:\t\t{i+1}
            Analyzed questions (Overlap):\t{question_count}
            ROUGE-1 Mean Score:\t\t{total_rouge/(i+1)}
          """)
    result[f"ROUGE-1-mean-epoch-{i+1}"] = total_rouge / (i + 1)
    print("=" * shutil.get_terminal_size().columns)

# Average the ROUGE-1 precision over the test epochs
mean_rouge = total_rouge / num_test_epoch
mean_question_count = question_count / num_test_epoch


print("\nEnd benchmark")
print("=" * shutil.get_terminal_size().columns)
print(f"""
        METRICS:
        
        Analyzed questions:\t{mean_question_count}
        ROUGE-1 Mean Score:\t{mean_rouge}
      """)
result["Epochs"] = num_test_epoch
result["Model name"] = model_name
result["Top-P"] = top_p
result["Temperature"] = temp
result["Max new tokens"] = n_token_out
result["Analyzed questions"] = mean_question_count
result["ROUGE-1 Mean Score"] = mean_rouge
result["Few-Shots"] = num_few_shots
now = datetime.datetime.now()
formatted_time = now.strftime("%Y-%m-%d_%H-%M-%S")
file_name = f"INVALSI_{model_name}_shots_{num_few_shots}_top-p_{top_p}_temp_{temp}_token_{n_token_out}_{formatted_time}.json"
with open(file_name, "w") as outfile:
    json.dump(dict(result), outfile, ensure_ascii=False)

print("=" * shutil.get_terminal_size().columns)
