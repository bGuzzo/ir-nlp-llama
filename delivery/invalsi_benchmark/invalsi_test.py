"""
This script defines and executes a benchmark for evaluating the performance of Large Language Models (LLMs)
on a dataset of INVALSI tests. INVALSI tests are standardized educational assessments in Italy, and this script
uses the Italian language and comprehension sections to measure how well an LLM performs on Italian language tasks.

The script loads a model, prepares a dataset of INVALSI test questions from JSON files, and then runs the model
against the questions. The performance is measured using the ROUGE-1 precision score, which evaluates the overlap
of unigrams (single words) between the model's generated answer and the correct answer.
"""

import datetime
import gc
import torch
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import os
import json
import shutil
from rouge_score import rouge_scorer

def calculate_rouge_1(ref_answer, llm_answer):
    """
    Calculates the ROUGE-1 precision score between a reference answer and a generated answer.

    Args:
        ref_answer (str): The correct or reference answer.
        llm_answer (str): The answer generated by the language model.

    Returns:
        float: The ROUGE-1 precision score.
    """
    scorer = rouge_scorer.RougeScorer(["rouge1"], use_stemmer=True)
    scores = scorer.score(ref_answer, llm_answer)
    return scores["rouge1"].precision

def parse_json_files(folder_path):
    """Parses all JSON files in a given folder and returns their content as a dictionary."""
    data = {}
    for filename in os.listdir(folder_path):
        if filename.endswith(".json"):
            filepath = os.path.join(folder_path, filename)
            with open(filepath, "r") as f:
                try:
                    data[filename] = json.load(f)
                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON in file {filename}: {e}")
    return data


print("=" * shutil.get_terminal_size().columns)
print("Start parsing INVALSI json file...\n")

# Path to the folder containing the INVALSI test data in JSON format.
folder_path = "/home/bruno/Documents/GitHub/ir-nlp-llama/delivery/invalsi_benchmark/json_tests"
parsed_data = parse_json_files(folder_path)

print(f"Loaded {len(parsed_data)} INVALSI test\n")
print("=" * shutil.get_terminal_size().columns)

# --- Model and Benchmark Configuration ---

# Select the model to evaluate.
base_model = "/home/bruno/Documents/GitHub/ir-nlp-llama/delivery/model_dump/Formal_LLaMAntino_3"
model_name = "Wiki-LLaMAntino"

# Configuration parameters for the benchmark.
top_p = 0.95
temp = 0.6
n_token_out = 4  # Max new tokens to generate, optimized for single-character answers.
num_test_epoch = 2  # Number of times to run the test to average out randomness.
num_few_shots = 2  # Number of few-shot examples to include in the prompt.

print(f"Initialize on model: f{model_name}")
print(f"Model HF path: f{base_model}")
print("=" * shutil.get_terminal_size().columns)
print("Start loading model...")

# Configure 4-bit quantization for memory efficiency.
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=False,
)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(base_model)

print("Model loaded successfully")
print("=" * shutil.get_terminal_size().columns)
print("\n")

print("=" * shutil.get_terminal_size().columns)
print("Start benchmark")

# Few-shot examples to guide the model's response format and style.
# These examples use summarized contexts to avoid exceeding the maximum sequence length.
few_shots = [
    # Few Shot 1
    """
    Contesto: 
        Il racconto narra la storia di un'amicizia estiva tra due ragazzi...

    Domanda:
        Secondo il narratore, perché l'amico, scaricando i bagagli, ha pianto?
        A. Ha litigato a lungo con il padre
        B. È stato costretto a fare un lavoro che non gli piace
        C. Il padre è molto severo con lui
        D. Gli è stato sottratto del tempo riservato alla vacanza
    
    D.
    """,
    # Few Shot 2
    """
    Contesto:
        Il testo descrive la fragilità degli ecosistemi forestali tropicali...

    Domanda:
        Nella frase "adattatisi a ruoli molto particolari" (riga 5), puoi sostituire "adattatisi" con:
        A. poiché si sono adattati
        B. prima che si siano adattati
        C. che si sono adattati
        D. nello stesso tempo si sono adattati
    
    C.
    """
]

def run_invalsi_test():
    """Runs a single epoch of the INVALSI test benchmark."""
    total_rouge = 0.0
    question_count = 0
    for file_name in parsed_data:
        print(f" Working file {file_name}")
        for invalsi_test in parsed_data[file_name]:
            test_context = invalsi_test["contesto"]
            for questions in invalsi_test["domande"]:
                test_question = questions["prompt_domanda"]
                right_answer = questions["risposta_corretta"]

                # Construct the few-shot examples string.
                prompt_shots = ""
                if num_few_shots > 0:
                    shots = "\n\n".join(few_shots[:num_few_shots])
                    prompt_shots = f"""
                    Inizio Esempi
                        {shots}
                    Fine Esempi
                    """

                # Construct the final prompt for the model.
                model_prompt = f"""
                    Informazioni: 
                        Rispondi alla seguente domanda.
                        Non indicare voci correlate e altre informazioni.
                        Fornisci in output solo la lettera associata alla risposta corretta.
                        
                    Esempio di output generato: "A.", "B.", "C." o "D."
                    
                    {prompt_shots}
                    
                    Rispondi alla seguente domanda.
                    
                    Contesto: {test_context}
                    
                    Domanda: {test_question}
                """

                # Create a text generation pipeline.
                pipe = transformers.pipeline(
                    model=model,
                    tokenizer=tokenizer,
                    return_full_text=False,
                    task="text-generation",
                    max_new_tokens=n_token_out,
                    temperature=temp,
                    do_sample=True,
                    top_p=top_p,
                )
                llm_reponse = pipe(model_prompt)
                full_reponse = "".join([seq["generated_text"] for seq in llm_reponse])
                
                # Clean the model's response.
                full_reponse = full_reponse.strip().replace(".", "")
                
                question_count += 1
                question_rouge = calculate_rouge_1(ref_answer=right_answer, llm_answer=full_reponse)
                total_rouge += question_rouge
                print(f"""
                    LLM Response:\t{full_reponse}
                    REAL Reponse:\t{right_answer}
                    ROUGE-1 Score: {question_rouge}
                    AVERAGE ROUGE-1 Score: {total_rouge/question_count}
                """)
                
                # Clean up memory.
                del pipe, full_reponse, llm_reponse
                gc.collect()
                torch.cuda.empty_cache()

    return {"rouge": total_rouge / question_count, "count": question_count}


# --- Main Execution ---

result = {}
total_rouge = 0.0
question_count = 0

# Run the benchmark for the specified number of epochs.
for i in range(num_test_epoch):
    print(f"Test epoch {i+1}")
    run_result = run_invalsi_test()
    total_rouge += run_result["rouge"]
    question_count += run_result["count"]
    print("=" * shutil.get_terminal_size().columns)
    print(f"""
            Epoch:\t\t{i+1}
            Analyzed questions (Overlap):\t{question_count}
            ROUGE-1 Mean Score:\t\t{total_rouge/(i+1)}
          """)
    result[f"ROUGE-1-mean-epoch-{i+1}"] = total_rouge / (i + 1)
    print("=" * shutil.get_terminal_size().columns)

# Calculate and print the final mean scores.
mean_rouge = total_rouge / num_test_epoch
mean_question_count = question_count / num_test_epoch

print("\nEnd benchmark")
print("=" * shutil.get_terminal_size().columns)
print(f"""
        METRICS:
        
        Analyzed questions:\t{mean_question_count}
        ROUGE-1 Mean Score:\t{mean_rouge}
      """)

# Save the results to a JSON file.
result["Epochs"] = num_test_epoch
result["Model name"] = model_name
result["Top-P"] = top_p
result["Temperature"] = temp
result["Max new tokens"] = n_token_out
result["Analyzed questions"] = mean_question_count
result["ROUGE-1 Mean Score"] = mean_rouge
result["Few-Shots"] = num_few_shots
now = datetime.datetime.now()
formatted_time = now.strftime("%Y-%m-%d_%H-%M-%S")
file_name = f"INVALSI_{model_name}_shots_{num_few_shots}_top-p_{top_p}_temp_{temp}_token_{n_token_out}_{formatted_time}.json"
with open(file_name, "w") as outfile:
    json.dump(dict(result), outfile, ensure_ascii=False)

print("=" * shutil.get_terminal_size().columns)
