\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and
  Guestrin]{chen2016trainingdeepnetssublinear}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost, 2016.
\newblock URL \url{https://arxiv.org/abs/1604.06174}.

\bibitem[Dac~Lai et~al.(2023)Dac~Lai, Van~Nguyen, Ngo, Nguyen, Dernoncourt,
  Rossi, and Nguyen]{dac2023okapi}
Viet Dac~Lai, Chien Van~Nguyen, Nghia~Trung Ngo, Thuat Nguyen, Franck
  Dernoncourt, Ryan~A Rossi, and Thien~Huu Nguyen.
\newblock Okapi: Instruction-tuned large language models in multiple languages
  with reinforcement learning from human feedback.
\newblock \emph{arXiv e-prints}, pages arXiv--2307, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.16039}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin2019bertpretrainingdeepbidirectional}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding, 2019.
\newblock URL \url{https://arxiv.org/abs/1810.04805}.

\bibitem[Douze et~al.(2024)Douze, Guzhva, Deng, Johnson, Szilvasy, Mazaré,
  Lomeli, Hosseini, and Jégou]{douze2024faisslibrary}
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,
  Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou.
\newblock The faiss library, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.08281}.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021loralowrankadaptationlarge}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.09685}.

\bibitem[Lewis et~al.(2021)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  Küttler, Lewis, tau Yih, Rocktäschel, Riedel, and
  Kiela]{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim
  Rocktäschel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks,
  2021.
\newblock URL \url{https://arxiv.org/abs/2005.11401}.

\bibitem[Llama~Team(2024)]{dubey2024llama3herdmodels}
AI@Meta Llama~Team.
\newblock The llama 3 herd of models, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.21783}.

\bibitem[Polignano et~al.(2024)Polignano, Basile, and
  Semeraro]{polignano2024advanced}
Marco Polignano, Pierpaolo Basile, and Giovanni Semeraro.
\newblock Advanced natural-based interaction for the italian language:
  Llamantino-3-anita, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.07101}.

\bibitem[Su et~al.(2023)Su, Lu, Pan, Murtadha, Wen, and
  Liu]{su2023roformerenhancedtransformerrotary}
Jianlin Su, Yu~Lu, Shengfeng Pan, Ahmed Murtadha, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding, 2023.
\newblock URL \url{https://arxiv.org/abs/2104.09864}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{touvron2023llamaopenefficientfoundation}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample.
\newblock Llama: Open and efficient foundation language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Vaswani et~al.(2023)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2023attentionneed}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2023.
\newblock URL \url{https://arxiv.org/abs/1706.03762}.

\bibitem[Wang et~al.(2020)Wang, Wei, Dong, Bao, Yang, and
  Zhou]{wang2020minilmdeepselfattentiondistillation}
Wenhui Wang, Furu Wei, Li~Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
\newblock Minilm: Deep self-attention distillation for task-agnostic
  compression of pre-trained transformers, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.10957}.

\bibitem[Zhang and Sennrich(2019)]{zhang2019rootmeansquarelayer}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization, 2019.
\newblock URL \url{https://arxiv.org/abs/1910.07467}.

\end{thebibliography}
