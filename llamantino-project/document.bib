@misc{polignano2024advanced,
	title={Advanced Natural-based interaction for the ITAlian language: LLaMAntino-3-ANITA}, 
	author={Marco Polignano and Pierpaolo Basile and Giovanni Semeraro},
	year={2024},
	eprint={2405.07101},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2405.07101},
}

@article{dac2023okapi,
	title={Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback},
	author={Dac Lai, Viet and Van Nguyen, Chien and Ngo, Nghia Trung and Nguyen, Thuat and Dernoncourt, Franck and Rossi, Ryan A and Nguyen, Thien Huu},
	journal={arXiv e-prints},
	pages={arXiv--2307},
	year={2023},
	url={https://arxiv.org/abs/2307.16039},
}

@misc{dubey2024llama3herdmodels,
	title={The Llama 3 Herd of Models}, 
	author={Llama Team, AI@Meta},
	year={2024},
	eprint={2407.21783},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2407.21783}, 
}

@misc{hu2021loralowrankadaptationlarge,
	title={LoRA: Low-Rank Adaptation of Large Language Models}, 
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2106.09685}, 
}

@misc{su2023roformerenhancedtransformerrotary,
	title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
	author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
	year={2023},
	eprint={2104.09864},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2104.09864}, 
}

@misc{zhang2019rootmeansquarelayer,
	title={Root Mean Square Layer Normalization}, 
	author={Biao Zhang and Rico Sennrich},
	year={2019},
	eprint={1910.07467},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1910.07467}, 
}

@misc{touvron2023llamaopenefficientfoundation,
	title={LLaMA: Open and Efficient Foundation Language Models}, 
	author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	year={2023},
	eprint={2302.13971},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2302.13971}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year={2019},
	eprint={1810.04805},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1810.04805}, 
}

@misc{vaswani2023attentionneed,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2023},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1706.03762}, 
}

@misc{wang2020minilmdeepselfattentiondistillation,
	title={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers}, 
	author={Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},
	year={2020},
	eprint={2002.10957},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2002.10957}, 
}


@misc{douze2024faisslibrary,
	title={The Faiss library}, 
	author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
	year={2024},
	eprint={2401.08281},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2401.08281}, 
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
	title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
	author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
	year={2021},
	eprint={2005.11401},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2005.11401}, 
}

@misc{chen2016trainingdeepnetssublinear,
	title={Training Deep Nets with Sublinear Memory Cost}, 
	author={Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin},
	year={2016},
	eprint={1604.06174},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1604.06174}, 
}
























